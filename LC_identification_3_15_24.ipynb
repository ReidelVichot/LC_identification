{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReidelVichot/LC_identification/blob/main/LC_identification_3_15_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. PROBLEM DEFINITION"
      ],
      "metadata": {
        "id": "Rqdn9j66FRnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background**\n",
        "\n",
        "“A logistics cluster (LC) is defined as the geographical concentration of firms providing logistics services, such as transportation carriers, warehousing providers, third-party logistics (3PL-s), and forwarders, as well as some other enterprises that are mainly in the logistics business, including logistics enterprises to provide services to various industries” (Rivera et al., 2014, p. 223).  \n",
        "\n",
        "Several relevant scholars in the field of logistics claim that clustering logistic activity has a positive impact on the efficiency of the economic activity, reduction of costs, and increase of collaboration among the firms that belong to the cluster (Rivera et al., 2014; Rivera, Gligor, et al., 2016; Rivera, Sheffi, et al., 2016; Sheffi, 2013, 2012). Although some of these authors mention that some of these benefits require some trade-offs (Rivera, Gligor, et al., 2016), these trade-offs are not further explored, resulting in an incomplete understanding of the socio-economic effects of the agglomeration of logistics activity. This becomes more problematic given that governments around the world seem to be embracing the idea of logistics clusters being some sort of panacea for economic development based on supply chain management improvements (Baranowski et al., 2015; Baydar et al., 2019; Chung, 2016), even though empirical studies that assess the role of government spending on the formation of logistics clusters are lacking (Liu et al., 2022). In other words, the field still lacks methodological and theoretical development, resulting in an incomplete understanding of the mechanisms of logistical clustering and their socio-economic effects.\n",
        "\n",
        "**Problem**\n",
        "\n",
        "There is not a current database of logistics clusters in the US. However, Rivera et al (2014) designed a method to test logistical agglomeration in US counties using NAICS codes and [CBP](https://www.census.gov/programs-surveys/cbp.html) information. Before conducting analyis on the effects of Logistics Clusters on society and the role of governments in their formation it is necessary to have an accurate picture of all logistics clusters in the US. For this purpose, I will extend Reviera's et al (2014) methodology to all the CBP years in which NAICS codes are used and use this database for future analyses."
      ],
      "metadata": {
        "id": "LvmLdFxsFRTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. DATA COLLECTION"
      ],
      "metadata": {
        "id": "AIyjkt-fFTKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RylmOoWgr-bA",
        "outputId": "ebfe10e1-22d4-4a0f-b730-014b9f958773"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import time\n",
        "\n",
        "# -- this line is to make pandas future-proof, Copy-on-Write will become the default in pandas 3.0.\n",
        "pd.options.mode.copy_on_write = True\n",
        "\n",
        "# -- Set the data path\n",
        "dpath = \"/content/drive/MyDrive/Disertation/\""
      ],
      "metadata": {
        "id": "TYXSF8rbsAos"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- NAICS of interest according to Rivera et al.\n",
        "naics_of_interest = [ \"481112\", \"481212\", \"481219\", \"483111\", \"483113\",\n",
        "                      \"483211\", \"484110\", \"484121\", \"484122\", \"484220\",\n",
        "                      \"484230\", \"488119\", \"488190\", \"488210\", \"488310\",\n",
        "                      \"488320\", \"488330\", \"488390\", \"488410\", \"488490\",\n",
        "                      \"488510\", \"488991\", \"488999\", \"492110\", \"492210\",\n",
        "                      \"493110\", \"493190\"]\n",
        "t0 = time.time()\n",
        "for year in range(1998, 2022):\n",
        "  xx = str(year)[2:]\n",
        "  fname = dpath + \"CBP_data/cbp\" + xx + \"co/cbp\" + xx + \"co.txt\"\n",
        "  temp = pd.read_csv(fname)\n",
        "  if year == 2015:\n",
        "    temp.columns = temp.columns.str.lower()\n",
        "  # -- add a year column\n",
        "  temp[\"year\"] = year\n",
        "  # -- add a GEOID\n",
        "  temp[\"GEOID\"] = temp.fipstate.astype(str).str.zfill(2) + temp.fipscty.astype(str).str.zfill(3)\n",
        "  # -- create a global variable and save a dataframe into it\n",
        "  globals()[\"cbp\" + xx] = temp[temp[\"naics\"].isin(naics_of_interest)]\n",
        "  # -- create a global variable and save the total employment for each county\n",
        "  globals()[\"cbp\" + xx + \"tot\"] = temp[temp[\"naics\"] == \"------\"]\n",
        "  # -- delete to save RAM\n",
        "  del temp\n",
        "t1 = time.time()\n",
        "print(\"Execution Time: \", (t1 - t0)/60, \" mins\")\n"
      ],
      "metadata": {
        "id": "Z3YszK3HbaOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13eaed7a-4823-4e2a-dbed-777577a62f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  3.551813844839732  mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. DATA PREPARATION"
      ],
      "metadata": {
        "id": "bbhRsKK6FTrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The CBP record layouts are different accross years.\n",
        "cols_98_16 = ['fipstate', 'fipscty', 'naics', 'empflag', 'emp', 'ap', 'est',\n",
        "              'n1_4', 'n5_9', 'n10_19', 'n20_49', 'n50_99', 'n100_249', 'n250_499',\n",
        "              'n500_999', 'n1000_1', 'n1000_2', 'n1000_3', 'n1000_4',\n",
        "              'year', 'GEOID']\n",
        "\n",
        "cols_17    = ['fipstate', 'fipscty', 'naics', 'empflag', 'emp', 'ap', 'est',\n",
        "              'n<5', 'n5_9', 'n10_19', 'n20_49', 'n50_99', 'n100_249', 'n250_499',\n",
        "              'n500_999', 'n1000_1', 'n1000_2', 'n1000_3', 'n1000_4',\n",
        "              'year', 'GEOID']\n",
        "\n",
        "cols_18_21 = ['fipstate', 'fipscty', 'naics', 'emp_nf',  'emp', 'ap', 'est',\n",
        "              'n<5', 'n5_9', 'n10_19', 'n20_49', 'n50_99', 'n100_249', 'n250_499',\n",
        "              'n500_999', 'n1000_1', 'n1000_2', 'n1000_3', 'n1000_4',\n",
        "              'year', 'GEOID']\n",
        "\n",
        "t0 = time.time()\n",
        "for year in range(1998, 2017):\n",
        "  xx = str(year)[2:]\n",
        "  globals()[\"cbp\" + xx] = globals()[\"cbp\" + xx][cols_98_16]\n",
        "  globals()[\"cbp\" + xx + \"tot\"] = globals()[\"cbp\" + xx + \"tot\"][cols_98_16]\n",
        "\n",
        "cbp17 = cbp17[cols_17]\n",
        "cbp17.columns = cbp98.columns\n",
        "cbp17tot = cbp17tot[cols_17]\n",
        "cbp17tot.columns = cbp98.columns\n",
        "\n",
        "#####  NOTE: CBP17 has object-type columns for the employment by establishment size.\n",
        "# from 2017-2021 there are values that have N instead of 0 for emp by est size\n",
        "cbp17[cbp17.columns[7:19]] = cbp17[cbp17.columns[7:19]].replace(\"N\", 0).astype(int)\n",
        "cbp17tot[cbp17tot.columns[7:19]] = cbp17tot[cbp17tot.columns[7:19]].replace(\"N\", 0).astype(int)\n",
        "\n",
        "for year in range(2018, 2022):\n",
        "  xx = str(year)[2:]\n",
        "  globals()[\"cbp\" + xx] = globals()[\"cbp\" + xx][cols_18_21]\n",
        "  globals()[\"cbp\" + xx][globals()[\"cbp\" + xx].columns[7:19]] = globals()[\"cbp\" + xx][globals()[\"cbp\" + xx].columns[7:19]].replace(\"N\", 0).astype(int)\n",
        "  globals()[\"cbp\" + xx].columns = cbp98.columns\n",
        "  globals()[\"cbp\" + xx + \"tot\"] = globals()[\"cbp\" + xx + \"tot\"][cols_18_21]\n",
        "  globals()[\"cbp\" + xx + \"tot\"][globals()[\"cbp\" + xx + \"tot\"].columns[7:19]] = globals()[\"cbp\" + xx + \"tot\"][globals()[\"cbp\" + xx + \"tot\"].columns[7:19]].replace(\"N\", 0).astype(int)\n",
        "  globals()[\"cbp\" + xx + \"tot\"].columns = cbp98.columns\n",
        "\n",
        "\n",
        "# -- Between 1998 and 2017, there was a suppression flag (empflag) that affects\n",
        "#    the employment. Instead of using employment as it is, I will adjust it\n",
        "#    based on the middle point of the employment by establishment size.\n",
        "\n",
        "\n",
        "\n",
        "# -- create an unified dataframe for all years (1998-2017)\n",
        "frames = [cbp98, cbp99, cbp00, cbp01, cbp02, cbp03, cbp04, cbp05, cbp06, cbp07,\n",
        "          cbp08, cbp09, cbp10, cbp11, cbp12, cbp13, cbp14, cbp15, cbp16, cbp17,\n",
        "          cbp18, cbp19, cbp20, cbp21]\n",
        "\n",
        "cbp = pd.concat(frames).reset_index().drop(columns=\"index\")\n",
        "# -- lambda funditon to estimate employment if the employment is flagged\n",
        "t0 = time.time()\n",
        "cbp['emp_adj'] = cbp.apply(lambda row: row[\"emp\"]\n",
        "                           if pd.isna(row['empflag'])\n",
        "                           else row['n1_4']*3 + row['n5_9']*7 +\n",
        "                                row['n10_19']*15 + row['n20_49']*35 +\n",
        "                                row['n50_99']*75 + row['n100_249']*175 +\n",
        "                                row['n250_499']*375 + row['n500_999']*750 +\n",
        "                                row['n1000_1']*1250 + row['n1000_2']*2000 +\n",
        "                                row['n1000_3']*3760 + row['n1000_4']*5000, axis=1)\n",
        "cbp = cbp.drop(columns=['n1_4', 'n5_9', 'n10_19', 'n20_49', 'n50_99',\n",
        "                        'n100_249', 'n250_499', 'n500_999', 'n1000_1',\n",
        "                        'n1000_2', 'n1000_3', 'n1000_4'])\n",
        "cbp[\"emp_adj\"] = cbp.apply(lambda row: row[\"emp\"] if (row[\"year\"]==2018)|(row[\"year\"]==2019)|(row[\"year\"]==2020)|(row[\"year\"]==2021) else row[\"emp_adj\"], axis=1)\n",
        "# -- repeat for total employment\n",
        "frames_tot = [cbp98tot, cbp99tot, cbp00tot, cbp01tot, cbp02tot, cbp03tot,\n",
        "              cbp04tot, cbp05tot, cbp06tot, cbp07tot, cbp08tot, cbp09tot,\n",
        "              cbp10tot, cbp11tot, cbp12tot, cbp13tot, cbp14tot, cbp15tot,\n",
        "              cbp16tot, cbp17tot, cbp18tot, cbp19tot, cbp20tot, cbp21tot]\n",
        "\n",
        "cbp_tot = pd.concat(frames_tot).reset_index().drop(columns=\"index\")\n",
        "# -- lambda funditon to estimate employment if the employment is flagged\n",
        "\n",
        "cbp_tot['emp_adj'] = cbp_tot.apply(lambda row: row[\"emp\"]\n",
        "                           if pd.isna(row['empflag'])\n",
        "                           else row['n1_4']*3 + row['n5_9']*7 +\n",
        "                                row['n10_19']*15 + row['n20_49']*35 +\n",
        "                                row['n50_99']*75 + row['n100_249']*175 +\n",
        "                                row['n250_499']*375 + row['n500_999']*750 +\n",
        "                                row['n1000_1']*1250 + row['n1000_2']*2000 +\n",
        "                                row['n1000_3']*3760 + row['n1000_4']*5000, axis=1)\n",
        "\n",
        "cbp_tot = cbp_tot.drop(columns=['n1_4', 'n5_9', 'n10_19', 'n20_49', 'n50_99',\n",
        "                                'n100_249', 'n250_499', 'n500_999', 'n1000_1',\n",
        "                                'n1000_2', 'n1000_3', 'n1000_4'])\n",
        "cbp_tot[\"emp_adj\"] = cbp_tot.apply(lambda row: row[\"emp\"] if (row[\"year\"]==2018)|(row[\"year\"]==2019)|(row[\"year\"]==2020)|(row[\"year\"]==2021) else row[\"emp_adj\"], axis=1)\n",
        "\n",
        "\n",
        "# this cbp is from 1998-2021\n",
        "cbp = cbp.groupby([\"GEOID\", \"year\", \"fipstate\", \"fipscty\"]).sum(numeric_only=True).reset_index()\n",
        "cbp_tot = cbp_tot.groupby([\"GEOID\", \"year\", \"fipstate\", \"fipscty\"]).sum(numeric_only=True).reset_index()\n",
        "# -- save the cvs to use it later\n",
        "cbp.to_csv(dpath + \"/cbp.csv\", index = False)\n",
        "cbp_tot.to_csv(dpath + \"/cbp_tot.csv\", index = False)\n",
        "t1 = time.time()\n",
        "print(\"Execution Time: \", (t1-t0)/60, \" mins\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tngQkfbFrpfr",
        "outputId": "9641b2d0-a90f-4d84-f06a-a04826e06e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time:  0.5479361534118652  mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this line to get access to the most current version of the CBP\n",
        "cbp_temp = pd.read_csv(dpath + \"/cbp.csv\")\n",
        "cbp_temp[\"GEOID\"] = cbp_temp[\"GEOID\"].astype(str).apply(lambda x: x.zfill(5))\n",
        "cbp = cbp_temp\n",
        "cbp_temp = pd.read_csv(dpath + \"/cbp_tot.csv\")\n",
        "cbp_temp[\"GEOID\"] = cbp_temp[\"GEOID\"].astype(str).apply(lambda x: x.zfill(5))\n",
        "cbp_tot = cbp_temp\n",
        "del cbp_temp"
      ],
      "metadata": {
        "id": "f0JlxdnopzTB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns from the total employment dataframe to merge it later\n",
        "cbp_tot = cbp_tot.rename(columns={\"emp\":\"tot_emp\",\t\"ap\":\"tot_ap\",\t\"est\":\"tot_est\",\t\"emp_adj\": \"tot_emp_adj\"})\n",
        "# Eliminate columns that won't be required\n",
        "cbp_tot = cbp_tot.drop(columns=[\"fipstate\", \"fipscty\"])\n",
        "# Merge dataframes\n",
        "print(\"CBP: \", len(cbp))\n",
        "print(\"CBP total: \", len(cbp_tot))\n",
        "cbp = cbp.merge(cbp_tot, left_on=(\"GEOID\",\"year\"), right_on=(\"GEOID\", \"year\"), how= \"outer\").fillna(0)\n",
        "print(\"Merged dataset: \", len(cbp))\n",
        "\n",
        "# Get yearly totals\n",
        "cbp_tot_year = cbp.groupby(\"year\").sum(numeric_only = True)[[\"emp_adj\", \"tot_emp_adj\"]]\n",
        "# Rename columns before merging\n",
        "cbp_tot_year = cbp_tot_year.rename(columns={\"emp_adj\":\"country_sector\",\t\"tot_emp_adj\":\"country_total\"}).reset_index()\n",
        "# Merge dataframes\n",
        "cbp = cbp.merge(cbp_tot_year, left_on=\"year\", right_on=\"year\")\n",
        "\n",
        "# Total establishments of logistics and warehousing in the whole country per year\n",
        "cbp = cbp.merge(cbp.groupby(\"year\").sum(numeric_only=True)[[\"est\"]].reset_index().rename(columns={\"est\": \"est_country\"}), left_on=\"year\", right_on=\"year\")\n",
        "\n",
        "# -- Create variables\n",
        "# Logistics Establishments' Participation\n",
        "cbp[\"LEP\"] = cbp.est / cbp.est_country\n",
        "\n",
        "# Location Quotient\n",
        "cbp[\"LQ\"] = (cbp.emp_adj/cbp.country_sector) / (cbp.tot_emp_adj/cbp.country_total)\n",
        "\n",
        "# Horizontal Cluster Location Quotient\n",
        "cbp[\"emp_expd\"] = cbp.country_sector*cbp.tot_emp_adj/cbp.country_total\n",
        "cbp[\"HCLQ\"] = cbp.emp_adj - cbp.emp_expd"
      ],
      "metadata": {
        "id": "24MqyvOHmXLn",
        "outputId": "32fbd4d2-9918-4228-c621-fe79a75dcc81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBP:  72146\n",
            "CBP total:  76537\n",
            "Merged dataset:  76537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. MACHINE LEARNING"
      ],
      "metadata": {
        "id": "lcLQnk75FUC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "geo = gpd.read_file(\"/content/drive/MyDrive/Disertation/countyshp/tl_2010_us_county00/tl_2010_us_county00.shp\")\n",
        "cols = ['CNTYIDFP00','NAME00','geometry']\n",
        "geo = geo[cols]\n",
        "cbp = geo.merge(cbp, right_on = \"GEOID\", left_on = 'CNTYIDFP00').drop(columns = 'CNTYIDFP00')"
      ],
      "metadata": {
        "id": "lpQnOXW_LHoK"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. PROBLEM SOLUTION"
      ],
      "metadata": {
        "id": "PHET5dqCFUUi"
      }
    }
  ]
}